{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8920a1a31e974d92858f388ae0c2b02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DATASET = 'https://raw.githubusercontent.com/rubenros1795/iconicity/master/data/dataset/TimesSquareKiss-sift-corrected.tsv'\n",
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "socket.setdefaulttimeout(10)\n",
    "\n",
    "df = pd.read_csv(DATASET, sep ='\\t')\n",
    "sample = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "if not os.path.exists ('images') :\n",
    "    os.mkdir('images')\n",
    "\n",
    "for row in tqdm (sample.itertuples(),total = SAMPLE_SIZE):\n",
    "    if row.image_url_full != 'na' :\n",
    "        url = row.image_url_full\n",
    "    else:\n",
    "        url = row.image_url_partial\n",
    "    file_extension = url [url.rfind('.') :]\n",
    "    if '?' in file_extension :\n",
    "        file_extension = file_extension[:file_extension.find('?')]\n",
    "    try:\n",
    "        urlretrieve (url, \"images/{}{}\".format(row.Index,\n",
    "        file_extension ) )\n",
    "    except : # not the cleanest , but there are many errors to\n",
    "#         account for\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def custom_split_train_test_val(directory, sample_size=0.2):\n",
    "    subsets = ['train', 'test', 'val']\n",
    "    upper_dir = os.path.dirname(os.path.dirname(directory))\n",
    "    for subset in subsets:\n",
    "        if not os.path.exists(os.path.join(upper_dir, subset)):\n",
    "            os.mkdir(os.path.join(upper_dir, subset))\n",
    "    for label in os.listdir(directory):\n",
    "        path = os.path.join(directory, label)\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        images = [image for image in sorted(os.listdir(path)) if image[-3:] in ['jpg', 'gif', 'epg', 'png']]\n",
    "        size_train = int((1 - sample_size) * len(images))\n",
    "        size_test = int((len(images) - size_train) / 2)\n",
    "        \n",
    "        for subset in subsets:\n",
    "            assert not os.path.exists(os.path.join(upper_dir, subset, label)), \"Path already exists, delete it first\"\n",
    "            os.mkdir(os.path.join(upper_dir, subset, label))\n",
    "        for train_image in images[:size_train]:\n",
    "            copyfile(os.path.join(path, train_image), \n",
    "                    os.path.join(upper_dir, 'train', label, train_image))\n",
    "        for test_image in images[size_train:size_train + size_test]:\n",
    "            copyfile(os.path.join(path, test_image), \n",
    "                    os.path.join(upper_dir, 'test', label, test_image))\n",
    "        for val_image in images[size_train + size_test:]:\n",
    "            copyfile(os.path.join(path, val_image), \n",
    "                    os.path.join(upper_dir, 'val', label, val_image))\n",
    "\n",
    "custom_split_train_test_val('data\\data', sample_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 260 files belonging to 3 classes.\n",
      "Found 56 files belonging to 3 classes.\n",
      "Found 58 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "train_ds = image_dataset_from_directory(\n",
    "    'train',  \n",
    "    shuffle=True,\n",
    "    image_size= IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=8,\n",
    "    )\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    'test',\n",
    "    shuffle=False,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "     'val',\n",
    "    shuffle=False,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 224, 224, 3), (None, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meme', 'original', 'other']\n"
     ]
    }
   ],
   "source": [
    "labels = ['meme', 'original', 'other']\n",
    "n_classes = len(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "\n",
    "IMG_SHAPE = IMAGE_SIZE + (3,)\n",
    "base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "preprocessing (Lambda)       (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 126,352,259\n",
      "Trainable params: 102,764,547\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Lambda(preprocess_input, name='preprocessing', input_shape=IMG_SHAPE))\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                         min_delta=0, patience=7, verbose=0, \n",
    "                          mode='min', baseline=None, \n",
    "                      restore_best_weights=True)]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33/33 [==============================] - ETA: 0s - loss: 1.4203 - accuracy: 0.8692\n",
      "\n",
      "33/33 [==============================] - 54s 2s/step - loss: 1.4203 - accuracy: 0.8692 - val_loss: 2.6754 - val_accuracy: 0.7241\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.6604 - accuracy: 0.9192 - val_loss: 1.8587 - val_accuracy: 0.7241\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.1081 - accuracy: 0.9692 - val_loss: 1.7539 - val_accuracy: 0.7586\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0190 - accuracy: 0.9962 - val_loss: 2.0280 - val_accuracy: 0.7759\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0064 - accuracy: 0.9962 - val_loss: 1.7537 - val_accuracy: 0.7414\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( \n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5, \n",
    "   callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 840ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       meme       0.60      0.67      0.63         9\n",
      "   original       0.86      0.83      0.85        30\n",
      "      other       0.88      0.88      0.88        17\n",
      "\n",
      "avg / total       0.83      0.82      0.82        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_ds, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = np.argmax(np.concatenate([labels.numpy() for images, labels in test_ds.take(-1)]), axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, \n",
    "                            target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  3  0]\n",
      " [ 3 25  2]\n",
      " [ 1  1 15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73e4ece5952446eb76f44532435cde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/11192.jpg\n",
      "images/19719.jpg\n",
      "images/21519.jpg\n",
      "images/23748.jpg\n",
      "images/23992.jpg\n",
      "images/30685.jpg\n",
      "images/36029.jpg\n",
      "images/36424.jpg\n",
      "images/36566.jpg\n",
      "images/46627.jpg\n",
      "images/47429.jpg\n",
      "images/53003.jpg\n",
      "images/55438.jpg\n",
      "images/5843.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kelvin\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:930: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/59438.jpg\n",
      "images/62290.jpg\n",
      "images/62884.jpg\n",
      "images/62952.jpg\n",
      "images/9420.jpg\n",
      "\n",
      "54/54 [==============================] - 237s 4s/step\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, single_batch=False):\n",
    "    \"\"\"\n",
    "    Convert the image to an array\n",
    "    \"\"\"\n",
    "    \n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=IMAGE_SIZE\n",
    "        )\n",
    "\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    if single_batch:\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# change to the directory that contains your unlabelled images\n",
    "directory2predict = 'images'\n",
    "\n",
    "image_paths = [image_path.path for image_path in os.scandir(directory2predict)] # the image paths\n",
    "image_paths = [image for image in image_paths if image[-3:] in ['jpg', 'gif', 'epg', 'png']]\n",
    "\n",
    "processed_list = []\n",
    "good_paths = []\n",
    "for image_path in tqdm(image_paths):\n",
    "    try:\n",
    "        res = process_image(image_path)\n",
    "        processed_list.append(res)\n",
    "        good_paths.append(image_path)\n",
    "    except:\n",
    "        print(image_path)\n",
    "\n",
    "processed_images = np.array(processed_list)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices(processed_images).batch(64)\n",
    "probabilities = model.predict(data, verbose=1)\n",
    "\n",
    "# 1/1 [==============================] - 0s 804us/step\n",
    "\n",
    "image_classes = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image_classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
